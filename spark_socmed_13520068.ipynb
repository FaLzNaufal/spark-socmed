{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Social Media\n",
    "### Muhammad Naufal Satriandana 13520068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "url = \"hdfs://127.0.0.1:9000/socmed_input/\"\n",
    "out_url = \"hdfs://127.0.0.1:9000/socmed_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('instagram', '2022-01-22', 8),\n",
       " ('instagram', '2022-01-19', 20),\n",
       " ('instagram', '2022-01-08', 8),\n",
       " ('instagram', '2021-12-31', 16),\n",
       " ('instagram', '2021-12-29', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig = spark.read.json(f\"{url}instagram*.json\").rdd\n",
    "# map the lines to a key-value pair\n",
    "def map(line):\n",
    "    date = line['created_time']\n",
    "    # format date\n",
    "    date = datetime.datetime.fromtimestamp(int(date)).strftime('%Y-%m-%d')\n",
    "    return ((\"instagram\", date), 1)\n",
    "\n",
    "ig = ig.map(map)\n",
    "\n",
    "ig = ig.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "ig = ig.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "ig.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youtube', '2021-05-21', 2),\n",
       " ('youtube', '2021-05-18', 2),\n",
       " ('youtube', '2021-06-01', 2),\n",
       " ('youtube', '2021-05-14', 2),\n",
       " ('youtube', '2021-05-12', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt = spark.read.json(f\"{url}youtube*.json\").rdd\n",
    "# map the lines to a key-value pair\n",
    "def map(line):\n",
    "    snippet = line['snippet']\n",
    "    # check if comment is a reply\n",
    "    if snippet['topLevelComment'] is not None:\n",
    "        snippet = snippet['topLevelComment']['snippet']\n",
    "    # get date from snippet.publishedAt\n",
    "    date = snippet['publishedAt'][0:10]\n",
    "    return ((\"youtube\", date), 1)\n",
    "\n",
    "yt = yt.map(map)\n",
    "\n",
    "yt = yt.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "yt = yt.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "yt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter', '2021-12-12', 5),\n",
       " ('twitter', '2021-03-31', 1),\n",
       " ('twitter', '2021-03-23', 2),\n",
       " ('twitter', '2021-03-30', 1),\n",
       " ('twitter', '2021-03-24', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt = spark.read.json(f\"{url}twitter*.json\").rdd\n",
    "# map the lines to a key-value pair\n",
    "def map(line):\n",
    "    date = line['created_at']\n",
    "    # format date\n",
    "    date = datetime.datetime.strptime(date, '%a %b %d %H:%M:%S %z %Y').strftime('%Y-%m-%d')\n",
    "    return ((\"twitter\", date), 1)\n",
    "\n",
    "twt = twt.map(map)\n",
    "\n",
    "twt = twt.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "twt = twt.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "twt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', '2021-05-03', 2),\n",
       " ('facebook', '2021-05-04', 3),\n",
       " ('facebook', '2021-05-06', 1),\n",
       " ('facebook', '2021-10-22', 1),\n",
       " ('facebook', '2021-04-27', 34)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb = spark.read.json(f\"{url}facebook*.json\").rdd\n",
    "\n",
    "def map(line):\n",
    "    result = []\n",
    "    postDate = line[\"created_time\"][0:10]\n",
    "    comments = line[\"comments\"][\"data\"]\n",
    "    result.append(((\"facebook\", postDate), 1))\n",
    "    for comment in comments:\n",
    "        date = comment[\"created_time\"][0:10]\n",
    "        result.append(((\"facebook\", date), 1))\n",
    "    return result\n",
    "\n",
    "fb = fb.flatMap(map)\n",
    "\n",
    "fb = fb.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "fb = fb.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "fb.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('instagram', '2022-02-09'), 55),\n",
       " (('instagram', '2022-02-11'), 268),\n",
       " (('instagram', '2022-02-12'), 199),\n",
       " (('instagram', '2022-02-13'), 48),\n",
       " (('instagram', '2022-02-08'), 53)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others = spark.read.option(\"multiline\",\"true\").json(f\"{url}*.json.json\").rdd\n",
    "# map the lines to a key-value pair\n",
    "def map(line):\n",
    "    graphImages = line['GraphImages']\n",
    "    result = []\n",
    "    if(graphImages is not None):\n",
    "        for image in graphImages:\n",
    "            if image['taken_at_timestamp'] is not None:\n",
    "                date = datetime.datetime.fromtimestamp(image['taken_at_timestamp']).strftime('%Y-%m-%d')\n",
    "                result.append(((\"instagram\", date), 1))\n",
    "            if image['comments'] is not None:\n",
    "                comments = image['comments']['data']\n",
    "                for comment in comments:\n",
    "                    date = datetime.datetime.fromtimestamp(comment['created_at']).strftime('%Y-%m-%d')\n",
    "                    result.append(((\"instagram\", date), 1))\n",
    "    return result\n",
    "\n",
    "others = others.flatMap(map)\n",
    "\n",
    "others = others.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "others.map(lambda x: (x[0][0], x[0][1], x[1])).collect()\n",
    "\n",
    "others.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('instagram', '2022-01-22', 8),\n",
       " ('instagram', '2022-01-19', 20),\n",
       " ('instagram', '2022-01-08', 8),\n",
       " ('instagram', '2021-12-31', 16),\n",
       " ('instagram', '2021-12-29', 1),\n",
       " ('instagram', '2021-01-17', 12),\n",
       " ('instagram', '2022-01-26', 70),\n",
       " ('instagram', '2022-01-25', 23),\n",
       " ('instagram', '2021-03-14', 2),\n",
       " ('instagram', '2022-01-23', 9),\n",
       " ('instagram', '2022-01-16', 8),\n",
       " ('instagram', '2022-01-14', 17),\n",
       " ('instagram', '2022-01-13', 11),\n",
       " ('instagram', '2022-01-10', 12),\n",
       " ('instagram', '2022-01-09', 9),\n",
       " ('instagram', '2022-01-05', 18),\n",
       " ('instagram', '2022-01-03', 11),\n",
       " ('instagram', '2022-01-01', 8),\n",
       " ('instagram', '2021-03-22', 2),\n",
       " ('instagram', '2021-03-29', 1),\n",
       " ('instagram', '2021-01-16', 15),\n",
       " ('instagram', '2022-01-21', 18),\n",
       " ('instagram', '2022-01-20', 19),\n",
       " ('instagram', '2022-01-18', 18),\n",
       " ('instagram', '2022-01-12', 10),\n",
       " ('instagram', '2022-01-11', 12),\n",
       " ('instagram', '2022-01-04', 19),\n",
       " ('instagram', '2021-03-20', 38),\n",
       " ('instagram', '2021-03-21', 14),\n",
       " ('instagram', '2021-01-04', 1),\n",
       " ('instagram', '2021-01-20', 1),\n",
       " ('instagram', '2021-01-27', 1),\n",
       " ('instagram', '2021-07-16', 1),\n",
       " ('instagram', '2022-01-17', 17),\n",
       " ('instagram', '2022-01-15', 5),\n",
       " ('instagram', '2022-01-07', 18),\n",
       " ('instagram', '2022-01-06', 16),\n",
       " ('instagram', '2022-01-02', 8),\n",
       " ('instagram', '2021-12-30', 15),\n",
       " ('instagram', '2021-04-09', 1),\n",
       " ('instagram', '2021-04-22', 1),\n",
       " ('instagram', '2021-01-15', 4),\n",
       " ('instagram', '2021-01-18', 8),\n",
       " ('instagram', '2021-01-19', 2),\n",
       " ('instagram', '2022-01-27', 44),\n",
       " ('youtube', '2021-05-21', 2),\n",
       " ('youtube', '2021-05-18', 2),\n",
       " ('youtube', '2021-06-01', 2),\n",
       " ('youtube', '2021-05-14', 2),\n",
       " ('youtube', '2021-05-12', 1),\n",
       " ('youtube', '2021-05-11', 1),\n",
       " ('youtube', '2021-05-09', 1),\n",
       " ('youtube', '2021-06-29', 2),\n",
       " ('youtube', '2021-05-07', 1),\n",
       " ('youtube', '2021-05-06', 1),\n",
       " ('youtube', '2021-05-03', 7),\n",
       " ('youtube', '2021-04-27', 10),\n",
       " ('youtube', '2021-04-26', 11),\n",
       " ('youtube', '2021-04-25', 3),\n",
       " ('youtube', '2021-10-11', 2),\n",
       " ('youtube', '2021-07-23', 3),\n",
       " ('youtube', '2021-05-20', 1),\n",
       " ('youtube', '2021-05-16', 1),\n",
       " ('youtube', '2021-05-19', 6),\n",
       " ('youtube', '2021-05-15', 1),\n",
       " ('youtube', '2021-05-13', 1),\n",
       " ('youtube', '2021-05-08', 1),\n",
       " ('youtube', '2021-06-12', 10),\n",
       " ('youtube', '2021-05-05', 1),\n",
       " ('youtube', '2021-05-02', 4),\n",
       " ('youtube', '2021-05-01', 3),\n",
       " ('youtube', '2021-04-30', 5),\n",
       " ('youtube', '2021-04-29', 6),\n",
       " ('youtube', '2021-04-28', 10),\n",
       " ('youtube', '2021-05-31', 2),\n",
       " ('twitter', '2021-12-12', 5),\n",
       " ('twitter', '2021-03-31', 1),\n",
       " ('twitter', '2021-03-23', 2),\n",
       " ('twitter', '2021-03-30', 1),\n",
       " ('twitter', '2021-03-24', 1),\n",
       " ('twitter', '2021-03-25', 2),\n",
       " ('twitter', '2021-12-11', 52),\n",
       " ('twitter', '2021-12-10', 43),\n",
       " ('twitter', '2021-12-13', 1),\n",
       " ('twitter', '2021-03-26', 4),\n",
       " ('twitter', '2021-03-28', 1),\n",
       " ('twitter', '2021-03-27', 1),\n",
       " ('twitter', '2021-03-29', 1),\n",
       " ('facebook', '2021-05-03', 2),\n",
       " ('facebook', '2021-05-04', 3),\n",
       " ('facebook', '2021-05-06', 1),\n",
       " ('facebook', '2021-10-22', 1),\n",
       " ('facebook', '2021-04-27', 34),\n",
       " ('facebook', '2021-04-26', 53),\n",
       " ('facebook', '2021-04-25', 28),\n",
       " ('facebook', '2021-04-24', 55),\n",
       " ('facebook', '2021-04-23', 52),\n",
       " ('facebook', '2021-04-22', 31),\n",
       " ('facebook', '2021-04-21', 69),\n",
       " ('facebook', '2021-04-19', 22),\n",
       " ('facebook', '2021-04-18', 25),\n",
       " ('facebook', '2021-04-16', 30),\n",
       " ('facebook', '2021-04-17', 7),\n",
       " ('facebook', '2021-04-14', 31),\n",
       " ('facebook', '2021-05-21', 1),\n",
       " ('facebook', '2021-04-10', 62),\n",
       " ('facebook', '2021-04-09', 138),\n",
       " ('facebook', '2021-04-08', 18),\n",
       " ('facebook', '2021-04-07', 25),\n",
       " ('facebook', '2021-04-04', 4),\n",
       " ('facebook', '2021-06-02', 1),\n",
       " ('facebook', '2021-02-03', 3),\n",
       " ('facebook', '2021-02-04', 1),\n",
       " ('facebook', '2021-01-30', 17),\n",
       " ('facebook', '2021-01-28', 26),\n",
       " ('facebook', '2021-10-17', 1),\n",
       " ('facebook', '2021-01-25', 22),\n",
       " ('facebook', '2021-01-21', 36),\n",
       " ('facebook', '2021-01-23', 1),\n",
       " ('facebook', '2021-01-22', 2),\n",
       " ('facebook', '2021-02-05', 1),\n",
       " ('facebook', '2021-01-19', 105),\n",
       " ('facebook', '2021-01-18', 95),\n",
       " ('facebook', '2021-01-15', 65),\n",
       " ('facebook', '2021-01-16', 14),\n",
       " ('facebook', '2021-01-12', 6),\n",
       " ('facebook', '2021-01-10', 27),\n",
       " ('facebook', '2021-01-09', 8),\n",
       " ('facebook', '2021-01-02', 24),\n",
       " ('facebook', '2021-01-03', 6),\n",
       " ('facebook', '2021-05-01', 50),\n",
       " ('facebook', '2021-05-02', 6),\n",
       " ('facebook', '2021-04-30', 66),\n",
       " ('facebook', '2021-04-29', 91),\n",
       " ('facebook', '2021-05-05', 1),\n",
       " ('facebook', '2021-04-28', 41),\n",
       " ('facebook', '2021-05-19', 2),\n",
       " ('facebook', '2021-10-07', 1),\n",
       " ('facebook', '2021-04-20', 4),\n",
       " ('facebook', '2021-04-15', 89),\n",
       " ('facebook', '2021-04-13', 33),\n",
       " ('facebook', '2021-04-12', 23),\n",
       " ('facebook', '2021-04-11', 51),\n",
       " ('facebook', '2021-04-05', 26),\n",
       " ('facebook', '2021-04-02', 58),\n",
       " ('facebook', '2021-04-03', 5),\n",
       " ('facebook', '2021-04-01', 60),\n",
       " ('facebook', '2021-02-01', 58),\n",
       " ('facebook', '2021-02-02', 22),\n",
       " ('facebook', '2021-01-31', 30),\n",
       " ('facebook', '2021-01-29', 10),\n",
       " ('facebook', '2021-01-27', 67),\n",
       " ('facebook', '2021-01-26', 82),\n",
       " ('facebook', '2021-01-20', 47),\n",
       " ('facebook', '2021-01-17', 1),\n",
       " ('facebook', '2021-01-13', 66),\n",
       " ('facebook', '2021-01-14', 9),\n",
       " ('facebook', '2021-01-11', 21),\n",
       " ('facebook', '2021-01-08', 52),\n",
       " ('facebook', '2021-02-12', 1),\n",
       " ('facebook', '2021-01-07', 74),\n",
       " ('facebook', '2021-01-06', 48),\n",
       " ('facebook', '2021-01-05', 36),\n",
       " ('facebook', '2021-01-04', 40),\n",
       " ('facebook', '2021-01-01', 20)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = ig.union(yt.union(twt.union(fb)))\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+\n",
      "|social_media|      date|count|\n",
      "+------------+----------+-----+\n",
      "|   instagram|2022-01-22|    8|\n",
      "|   instagram|2022-01-19|   20|\n",
      "|   instagram|2022-01-08|    8|\n",
      "|   instagram|2021-12-31|   16|\n",
      "|   instagram|2021-12-29|    1|\n",
      "|   instagram|2021-01-17|   12|\n",
      "|   instagram|2022-01-26|   70|\n",
      "|   instagram|2022-01-25|   23|\n",
      "|   instagram|2021-03-14|    2|\n",
      "|   instagram|2022-01-23|    9|\n",
      "|   instagram|2022-01-16|    8|\n",
      "|   instagram|2022-01-14|   17|\n",
      "|   instagram|2022-01-13|   11|\n",
      "|   instagram|2022-01-10|   12|\n",
      "|   instagram|2022-01-09|    9|\n",
      "|   instagram|2022-01-05|   18|\n",
      "|   instagram|2022-01-03|   11|\n",
      "|   instagram|2022-01-01|    8|\n",
      "|   instagram|2021-03-22|    2|\n",
      "|   instagram|2021-03-29|    1|\n",
      "+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = RDD.toDF(['social_media', 'date', 'count'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://127.0.0.1:9000/sample_output already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o272.csv.\n: org.apache.spark.sql.AnalysisException: path hdfs://127.0.0.1:9000/sample_output already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:684)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-85814a92a2af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    930\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://127.0.0.1:9000/sample_output already exists.;'"
     ]
    }
   ],
   "source": [
    "df.write.csv(out_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
